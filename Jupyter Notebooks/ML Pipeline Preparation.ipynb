{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook work we will work on creating ML Pipeline \n",
    "- Import Necessary Python Modules\n",
    "- Load data from sqlite database created earlier\n",
    "- **Build , Train and Evaluate** your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\U6054057\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\U6054057\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\U6054057\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "###################    Import Python Modules    ###########################\n",
    "\n",
    "#To Handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#To handle Databases\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import string \n",
    "import sys \n",
    "\n",
    "#To Handle text data using Natural Language ToolKit\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet','stopwords'])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Sklearn Libraries for Ml Models\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load Dataset from sqlite database\n",
    "- Use `read_sql_table` to read data from DisasterResponse database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Weather update - a cold front from Cuba that c...\n",
      "1              Is the Hurricane over or is it not over\n",
      "2                      Looking for someone but no name\n",
      "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
      "4    says: west side of Haiti, rest of the country ...\n",
      "Name: message, dtype: object    related  request  offer  aid_related  medical_help  medical_products  \\\n",
      "0        1        0      0            0             0                 0   \n",
      "1        1        0      0            1             0                 0   \n",
      "2        1        0      0            0             0                 0   \n",
      "3        1        1      0            1             0                 1   \n",
      "4        1        0      0            0             0                 0   \n",
      "\n",
      "   search_and_rescue  security  military  water  ...  aid_centers  \\\n",
      "0                  0         0         0      0  ...            0   \n",
      "1                  0         0         0      0  ...            0   \n",
      "2                  0         0         0      0  ...            0   \n",
      "3                  0         0         0      0  ...            0   \n",
      "4                  0         0         0      0  ...            0   \n",
      "\n",
      "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
      "0                     0                0       0      0     0           0   \n",
      "1                     0                1       0      1     0           0   \n",
      "2                     0                0       0      0     0           0   \n",
      "3                     0                0       0      0     0           0   \n",
      "4                     0                0       0      0     0           0   \n",
      "\n",
      "   cold  other_weather  direct_report  \n",
      "0     0              0              0  \n",
      "1     0              0              0  \n",
      "2     0              0              0  \n",
      "3     0              0              0  \n",
      "4     0              0              0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "engine= create_engine('sqlite:///DisasterResponse.db')\n",
    "#\n",
    "df = pd.read_sql_table('DS_messages',engine)\n",
    "X=df['message']\n",
    "y=df[df.columns[4:]]\n",
    "print(X.head(),y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Function to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    INPUT - text - messages column from the table\n",
    "    Returns tokenized text \n",
    "    \n",
    "    1. Remove Punctuation and normalize text\n",
    "    2. Tokenize text and remove stop words\n",
    "    3.Use stemmer and Lemmatizer to Reduce words to its root form\n",
    "    \"\"\"\n",
    "    # Remove Punctuations and normalize text by converting text into lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize text and remove stop words\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    #Reduce words to its stem/Root form\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    #Lemmatizer - Reduce words to its root form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm = [lemmatizer.lemmatize(w) for w in stemmed]\n",
    "    \n",
    "    return lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Build Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have X as messages columns which is the input to the model and y as the 36 categories which results as output classification to our model. As we have multi features to classify we can make use of [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "]) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x0000025A971C2700>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Predict the model \n",
    "prints [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) from sklearn library which returns Precision,Recall and f1-score of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.95      0.89      7939\n",
      "               request       0.84      0.47      0.60      1811\n",
      "                 offer       0.00      0.00      0.00        48\n",
      "           aid_related       0.77      0.70      0.73      4378\n",
      "          medical_help       0.70      0.07      0.13       848\n",
      "      medical_products       0.83      0.09      0.16       530\n",
      "     search_and_rescue       0.71      0.02      0.03       288\n",
      "              security       0.20      0.01      0.01       188\n",
      "              military       0.61      0.06      0.10       341\n",
      "                 water       0.88      0.37      0.52       638\n",
      "                  food       0.86      0.54      0.66      1204\n",
      "               shelter       0.84      0.33      0.47       933\n",
      "              clothing       0.89      0.05      0.09       163\n",
      "                 money       0.78      0.03      0.05       250\n",
      "        missing_people       1.00      0.01      0.02       122\n",
      "              refugees       0.53      0.02      0.04       356\n",
      "                 death       0.79      0.14      0.24       478\n",
      "             other_aid       0.52      0.03      0.06      1415\n",
      "infrastructure_related       0.00      0.00      0.00       697\n",
      "             transport       0.74      0.05      0.10       482\n",
      "             buildings       0.76      0.12      0.20       522\n",
      "           electricity       0.65      0.05      0.10       212\n",
      "                 tools       0.00      0.00      0.00        68\n",
      "             hospitals       0.00      0.00      0.00       124\n",
      "                 shops       0.00      0.00      0.00        45\n",
      "           aid_centers       0.00      0.00      0.00       142\n",
      "  other_infrastructure       0.00      0.00      0.00       455\n",
      "       weather_related       0.84      0.73      0.78      2920\n",
      "                floods       0.90      0.54      0.67       850\n",
      "                 storm       0.78      0.50      0.61       993\n",
      "                  fire       0.00      0.00      0.00       109\n",
      "            earthquake       0.89      0.80      0.84       988\n",
      "                  cold       0.83      0.07      0.14       200\n",
      "         other_weather       0.57      0.01      0.03       552\n",
      "         direct_report       0.79      0.36      0.50      1988\n",
      "\n",
      "             micro avg       0.82      0.53      0.64     33277\n",
      "             macro avg       0.58      0.20      0.25     33277\n",
      "          weighted avg       0.75      0.53      0.57     33277\n",
      "           samples avg       0.67      0.48      0.51     33277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set \n",
    "y_pred = pipeline.predict(X_test)\n",
    "categories = y.columns.tolist()\n",
    "# Test model on test set\n",
    "print(classification_report(y_test, y_pred, target_names=categories, zero_division=\"warn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use KNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use KNN Classifier\n",
    "\n",
    "#create pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "]) \n",
    "\n",
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set \n",
    "y_pred = pipeline.predict(X_test)\n",
    "categories = y.columns.tolist()\n",
    "\n",
    "# Test model on test set\n",
    "print(classification_report(y_test, y_pred, target_names=categories, zero_division=\"warn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Improve Model\n",
    "**Use GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.222, total=  41.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.242, total=  43.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.224, total=  44.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.223, total=  45.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.231, total=  44.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.220, total=  45.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.221, total=  42.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.235, total=  43.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.215, total=  42.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.228, total=  40.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.243, total=  43.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.232, total=  45.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.224, total=  44.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.201, total=  45.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.232, total=  45.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.220, total=  46.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.230, total=  43.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.217, total=  46.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.222, total=  43.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.242, total=  44.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.224, total=  44.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.223, total=  45.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.231, total=  45.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.220, total=  45.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.221, total=  43.0s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.235, total=  44.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.215, total=  43.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.228, total=  41.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.243, total=  44.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.232, total=  44.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.224, total=  43.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.201, total=  44.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.232, total=  43.6s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.220, total=  44.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.230, total=  41.8s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.217, total=  44.2s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.222, total=  42.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.242, total=  43.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.224, total=  43.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.223, total=  45.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.231, total=  45.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.220, total=  46.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.221, total=  43.1s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.235, total=  43.7s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.215, total=  44.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.228, total=  41.5s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.243, total=  42.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.232, total=  44.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.224, total=  42.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.201, total=  44.3s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.232, total=  43.9s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.220, total=  44.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.230, total=  42.4s\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.217, total=  44.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.239, total=  44.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.250, total=  43.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.239, total=  44.8s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.233, total=  45.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.245, total=  40.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2), score=0.230, total=  40.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.236, total=  44.8s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.249, total=  45.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.235, total=  43.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.237, total=  44.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.250, total=  45.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.239, total=  45.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.241, total=  45.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.219, total=  43.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.250, total=  44.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.233, total=  43.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.244, total=  45.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.228, total=  44.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.239, total=  44.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.250, total=  43.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.239, total=  43.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.233, total=  44.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.245, total=  39.8s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2), score=0.230, total=  40.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.236, total=  45.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.249, total=  44.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.235, total=  44.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.237, total=  45.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.250, total=  45.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.239, total=  46.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.241, total=  45.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.219, total=  43.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.250, total=  45.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.233, total=  43.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.244, total=  46.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.228, total=  44.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.239, total=  43.8s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.250, total=  43.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1), score=0.239, total=  44.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.233, total=  45.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.245, total=  39.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2), score=0.230, total=  39.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.236, total=  43.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.249, total=  43.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.235, total=  43.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.237, total=  44.5s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.250, total=  45.1s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.239, total=  45.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.241, total=  44.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.219, total=  44.3s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.250, total=  45.0s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.233, total=  42.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.244, total=  45.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.228, total=  46.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 79.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x0000025A971C2700>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=KNeighborsClassifier()))]),\n",
       "             param_grid={'tfidf__use_idf': (True, False),\n",
       "                         'vect__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vect__max_features': (None, 5000, 10000),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1,2)),\n",
    "    'vect__max_features': (None, 5000,10000),\n",
    "    'tfidf__use_idf': (True, False)\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=3, verbose=3)\n",
    "\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.89      0.86      7939\n",
      "               request       0.64      0.50      0.56      1811\n",
      "                 offer       0.00      0.00      0.00        48\n",
      "           aid_related       0.67      0.55      0.61      4378\n",
      "          medical_help       0.52      0.11      0.19       848\n",
      "      medical_products       0.60      0.12      0.20       530\n",
      "     search_and_rescue       0.64      0.06      0.10       288\n",
      "              security       0.40      0.01      0.02       188\n",
      "              military       0.65      0.13      0.22       341\n",
      "                 water       0.67      0.31      0.42       638\n",
      "                  food       0.73      0.38      0.50      1204\n",
      "               shelter       0.65      0.25      0.36       933\n",
      "              clothing       0.60      0.15      0.24       163\n",
      "                 money       0.53      0.09      0.16       250\n",
      "        missing_people       0.60      0.02      0.05       122\n",
      "              refugees       0.57      0.08      0.14       356\n",
      "                 death       0.65      0.17      0.27       478\n",
      "             other_aid       0.32      0.08      0.13      1415\n",
      "infrastructure_related       0.20      0.02      0.04       697\n",
      "             transport       0.66      0.10      0.17       482\n",
      "             buildings       0.69      0.16      0.26       522\n",
      "           electricity       0.67      0.14      0.23       212\n",
      "                 tools       0.00      0.00      0.00        68\n",
      "             hospitals       0.00      0.00      0.00       124\n",
      "                 shops       0.00      0.00      0.00        45\n",
      "           aid_centers       0.00      0.00      0.00       142\n",
      "  other_infrastructure       0.14      0.01      0.02       455\n",
      "       weather_related       0.72      0.53      0.61      2920\n",
      "                floods       0.72      0.24      0.36       850\n",
      "                 storm       0.67      0.35      0.46       993\n",
      "                  fire       0.71      0.11      0.19       109\n",
      "            earthquake       0.76      0.54      0.63       988\n",
      "                  cold       0.58      0.12      0.21       200\n",
      "         other_weather       0.50      0.07      0.13       552\n",
      "         direct_report       0.58      0.42      0.49      1988\n",
      "\n",
      "             micro avg       0.73      0.46      0.57     33277\n",
      "             macro avg       0.51      0.19      0.25     33277\n",
      "          weighted avg       0.66      0.46      0.51     33277\n",
      "           samples avg       0.58      0.42      0.44     33277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set \n",
    "y_pred = pipeline.predict(X_test)\n",
    "categories = y.columns.tolist()\n",
    "\n",
    "# Test model on test set\n",
    "print(classification_report(y_test, y_pred, target_names=categories, zero_division=\"warn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "\n",
    "joblib.dump(cv, 'DS_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the help of this notebook we can build ML Pipeline in `train_classifie.py` Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.89      0.86      7939\n",
      "               request       0.64      0.50      0.56      1811\n",
      "                 offer       0.00      0.00      0.00        48\n",
      "           aid_related       0.67      0.55      0.61      4378\n",
      "          medical_help       0.52      0.11      0.19       848\n",
      "      medical_products       0.60      0.12      0.20       530\n",
      "     search_and_rescue       0.64      0.06      0.10       288\n",
      "              security       0.40      0.01      0.02       188\n",
      "              military       0.65      0.13      0.22       341\n",
      "                 water       0.67      0.31      0.42       638\n",
      "                  food       0.73      0.38      0.50      1204\n",
      "               shelter       0.65      0.25      0.36       933\n",
      "              clothing       0.60      0.15      0.24       163\n",
      "                 money       0.53      0.09      0.16       250\n",
      "        missing_people       0.60      0.02      0.05       122\n",
      "              refugees       0.57      0.08      0.14       356\n",
      "                 death       0.65      0.17      0.27       478\n",
      "             other_aid       0.32      0.08      0.13      1415\n",
      "infrastructure_related       0.20      0.02      0.04       697\n",
      "             transport       0.66      0.10      0.17       482\n",
      "             buildings       0.69      0.16      0.26       522\n",
      "           electricity       0.67      0.14      0.23       212\n",
      "                 tools       0.00      0.00      0.00        68\n",
      "             hospitals       0.00      0.00      0.00       124\n",
      "                 shops       0.00      0.00      0.00        45\n",
      "           aid_centers       0.00      0.00      0.00       142\n",
      "  other_infrastructure       0.14      0.01      0.02       455\n",
      "       weather_related       0.72      0.53      0.61      2920\n",
      "                floods       0.72      0.24      0.36       850\n",
      "                 storm       0.67      0.35      0.46       993\n",
      "                  fire       0.71      0.11      0.19       109\n",
      "            earthquake       0.76      0.54      0.63       988\n",
      "                  cold       0.58      0.12      0.21       200\n",
      "         other_weather       0.50      0.07      0.13       552\n",
      "         direct_report       0.58      0.42      0.49      1988\n",
      "\n",
      "             micro avg       0.73      0.46      0.57     33277\n",
      "             macro avg       0.51      0.19      0.25     33277\n",
      "          weighted avg       0.66      0.46      0.51     33277\n",
      "           samples avg       0.58      0.42      0.44     33277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\U6054057\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Use KNN Classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#create pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "]) \n",
    "\n",
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set \n",
    "y_pred = pipeline.predict(X_test)\n",
    "categories = y.columns.tolist()\n",
    "\n",
    "# Test model on test set\n",
    "print(classification_report(y_test, y_pred, target_names=categories, zero_division=\"0\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
